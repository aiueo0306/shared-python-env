ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¸ã¨ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
ãƒãƒ£ãƒƒãƒˆå±¥æ­´

ã‚ãªãŸ:
import re
from datetime import datetime, timezone
from urllib.parse import urljoin

def extract_items(page, SELECTOR_DATE, SELECTOR_TITLE, title_selector, title_index, href_selector, href_index, base_url, date_selector, date_index, date_format, date_regex, max_items=10):
    
    page.wait_for_selector(SELECTOR_TITLE, timeout=10000)
    
    blocks1 = page.locator(SELECTOR_TITLE)
    count = blocks1.count()
    
    print(f"ğŸ“¦ ç™ºè¦‹ã—ãŸè¨˜äº‹æ•°: {count}")
    items = []

    blocks2 = page.locator(SELECTOR_DATE)
    
    for i in range(min(count, max_items)):
        try:
            block1 = blocks1.nth(i)
            block2 = blocks2.nth(i)

            # ã‚¿ã‚¤ãƒˆãƒ«
            if title_selector:
                title_elem = block1.locator(title_selector).nth(title_index)
                title = title_elem.inner_text().strip()
            else:
                title = block1.inner_text().strip()
            print(title)


            
            # URL
            if title_selector:
                try:
                    href = block1.locator(href_selector).nth(href_index).get_attribute("href")
                    full_link = urljoin(base_url, href)
                except:
                    href = ""
                    full_link = base_url
            else:
                try:
                    href = block1.get_attribute("href")
                    full_link = urljoin(base_url, href)
                except:
                    href = ""
                    full_link = base_url
            print(full_link)
            
            # æ—¥ä»˜
            # date_selector ãŒç©ºæ–‡å­—ã‚„ None ã§ãªã„å ´åˆ â†’ å­è¦ç´ æ¢ç´¢ã€ãã‚Œä»¥å¤–ã¯ãã®ã¾ã¾
            if date_selector:
                try:
                    date_text = block2.locator(date_selector).nth(date_index).inner_text().strip()
                except Exception as e:
                    print(f"âš  æ—¥ä»˜ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã«ã‚ˆã‚‹å–å¾—ã«å¤±æ•—: {e}")
                    date_text = ""
            else:
                try:
                    date_text = block2.inner_text().strip()
                except Exception as e:
                    print(f"âš  ç›´æ¥æ—¥ä»˜å–å¾—ã«å¤±æ•—: {e}")
                    date_text = ""
            print(date_text)
            match = re.search(date_regex, date_text)

            if match:
                year_str, month_str, day_str = match.groups()
                year = int(year_str)
                if year < 100:
                    year += 2000  # 2æ¡è¥¿æš¦ â†’ 2000å¹´ä»¥é™ã¨ä»®å®š
                pub_date = datetime(year, int(month_str), int(day_str), tzinfo=timezone.utc)
            else:
                print("âš  æ—¥ä»˜ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                pub_date = None  # or continue
            
            # match = re.search(date_regex,date_text)
            # if match:
            #     date_str = match.group()
            #     pub_date = datetime.strptime(date_str, date_format).replace(tzinfo=timezone.utc)
            # else:
            #     print("âš  æ—¥ä»˜ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")

            print(pub_date)
            
            if not title or not href:
                print(f"âš  å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæ¬ è½ã—ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ{i+1}è¡Œç›®ï¼‰: title='{title}', href='{href}'")
                continue

            items.append({
                "title": title,
                "link": full_link,
                "description": title,
                "pub_date": pub_date
            })

        except Exception as e:
            print(f"âš  è¡Œ{i+1}ã®è§£æã«å¤±æ•—: {e}")
            continue

    return items

ã“ã®ã‚³ãƒ¼ãƒ‰ã§æ—¥ä»˜å–å¾—ãŒã§ããªã‹ã£ãŸå ´åˆã¯ä»Šæ—¥ã®puddateã¯Noneã«ã—ã¦ã€æ¬¡ã®å‡¦ç†ã«å›ã—ãŸã„
ChatGPT:
ã“ã®å ´åˆã€æ—¥ä»˜ãŒå–å¾—ã§ããªã‹ã£ãŸã¨ãã« pub_date ã‚’ None ã«ã—ã¦ã€ãã®ã¾ã¾æ¬¡ã®å‡¦ç†ã«å›ã™ã‚ˆã†ã«ã™ã‚‹ãªã‚‰ã€if match: ã® else: å†…ã§ pub_date = None ã‚’æ˜ç¤ºã™ã‚Œã°OKã§ã™ã€‚
ä¾‹ãˆã°ã“ã†æ›¸ãæ›ãˆã‚Œã°æ„å›³é€šã‚Šå‹•ãã¾ã™ã€‚

# æ—¥ä»˜è§£æ
match = re.search(date_regex, date_text)

if match:
    year_str, month_str, day_str = match.groups()
    year = int(year_str)
    if year < 100:
        year += 2000  # 2æ¡è¥¿æš¦ã¯2000å¹´ä»¥é™ã¨ä»®å®š
    pub_date = datetime(year, int(month_str), int(day_str), tzinfo=timezone.utc)
else:
    print("âš  æ—¥ä»˜ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
    pub_date = None  # å–å¾—ã§ããªã‹ã£ãŸã‚‰ None ã«ã™ã‚‹

print(pub_date)
ã“ã®ã‚ˆã†ã«ã™ã‚‹ã¨ã€æ—¥ä»˜ã®æŠ½å‡ºã«å¤±æ•—ã—ã¦ã‚‚ pub_date ã¯ None ã§ä¿æŒã•ã‚Œã€
ãã®å¾Œã®

items.append({
    "title": title,
    "link": full_link,
    "description": title,
    "pub_date": pub_date
})
ã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚
continue ã›ãš None ã®ã¾ã¾æ¬¡ã¸é€²ã‚€ã®ã§ã€å¾Œç¶šå‡¦ç†å´ã§ None ã‚’è¦‹ã¦ã€Œæ—¥ä»˜ãªã—ã€ã¨åˆ¤æ–­ã™ã‚Œã°OKã§ã™ã€‚

ã‚‚ã—å¸Œæœ›ãŒã‚ã‚Œã°ã€ã“ã® extract_items() é–¢æ•°ã‚’ä¾‹å¤–å‡¦ç†ã‚‚å«ã‚ã¦ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ã§å®‰å…¨ã«æ›¸ãç›´ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚‚ä½œã‚Œã¾ã™ã€‚

ã‚ãªãŸ:
Noneã«ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®é–¢æ•°ã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã‚ˆã­ï¼Ÿ

from feedgen.feed import FeedGenerator
from datetime import datetime, timezone
import os

def generate_rss(items, output_path, base_url, gakkai_name):
    fg = FeedGenerator()
    fg.title(f"{gakkai_name}ãƒˆãƒ”ãƒƒã‚¯ã‚¹")
    fg.link(href=base_url)
    fg.description(f"{gakkai_name}ã®æœ€æ–°ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±")
    fg.language("ja")
    fg.generator("python-feedgen")
    fg.docs("http://www.rssboard.org/rss-specification")
    fg.lastBuildDate(datetime.now(timezone.utc))

    for item in items:
        entry = fg.add_entry()
        entry.title(item['title'])
        entry.link(href=item['link'])
        entry.description(item['description'])
        guid_value = f"{item['link']}#{item['pub_date'].strftime('%Y%m%d')}"
        entry.guid(guid_value, permalink=False)
        entry.pubDate(item['pub_date'])

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fg.rss_file(output_path)
    print(f"\nâœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼ğŸ“„ ä¿å­˜å…ˆ: {output_path}")
ChatGPT:
ã¯ã„ã€ãã®ã¾ã¾ None ã‚’å…¥ã‚Œã‚‹ã¨ã€

item['pub_date'].strftime('%Y%m%d')
ã®éƒ¨åˆ†ã§ AttributeError: 'NoneType' object has no attribute 'strftime' ã«ãªã‚Šã¾ã™ã€‚

ãªã®ã§ã€generate_rss() å´ã§ pub_date ãŒ None ã®å ´åˆã®å‡¦ç†ã‚’åˆ†ã‘ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ä¾‹ãˆã°ã€pub_date ãŒ None ã®å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã€Œæ—¥ä»˜ãªã—ã€ç”¨ã®ä»£æ›¿å€¤ã‚’è¨­å®šã§ãã¾ã™ã€‚

def generate_rss(items, output_path, base_url, gakkai_name):
    fg = FeedGenerator()
    fg.title(f"{gakkai_name}ãƒˆãƒ”ãƒƒã‚¯ã‚¹")
    fg.link(href=base_url)
    fg.description(f"{gakkai_name}ã®æœ€æ–°ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±")
    fg.language("ja")
    fg.generator("python-feedgen")
    fg.docs("http://www.rssboard.org/rss-specification")
    fg.lastBuildDate(datetime.now(timezone.utc))

    for item in items:
        entry = fg.add_entry()
        entry.title(item['title'])
        entry.link(href=item['link'])
        entry.description(item['description'])

        if item['pub_date'] is not None:
            guid_value = f"{item['link']}#{item['pub_date'].strftime('%Y%m%d')}"
            entry.pubDate(item['pub_date'])
        else:
            # æ—¥ä»˜ãŒç„¡ã„å ´åˆã¯ä»Šæ—¥ã®æ—¥ä»˜ã‚’ä½¿ã†ï¼ˆã¾ãŸã¯å›ºå®šå€¤ã§ã‚‚OKï¼‰
            today = datetime.now(timezone.utc)
            guid_value = f"{item['link']}#{today.strftime('%Y%m%d')}"
            entry.pubDate(today)

        entry.guid(guid_value, permalink=False)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fg.rss_file(output_path)
    print(f"\nâœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼ğŸ“„ ä¿å­˜å…ˆ: {output_path}")
ã“ã†ã™ã‚Œã° extract_items() ã§ pub_date=None ã«ã—ã¦ã‚‚å®‰å…¨ã«å‡¦ç†ã§ãã¾ã™ã—ã€RSSå´ã§ã¯æ—¥ä»˜ãŒãªã„è¨˜äº‹ã«ã‚‚ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã•ãšã«å¯¾å¿œã§ãã¾ã™ã€‚

ã‚‚ã—ã€Œæ—¥ä»˜ãŒãªã„å ´åˆã¯ RSS ã«å«ã‚ãªã„ã€ã¨ã„ã†ä»•æ§˜ã«ã—ãŸã„ãªã‚‰ã€
generate_rss() ã®ãƒ«ãƒ¼ãƒ—å†…ã§ if item['pub_date'] is None: continue ã¨ã—ã¦ã‚¹ã‚­ãƒƒãƒ—ã‚‚å¯èƒ½ã§ã™ã€‚

ã©ã¡ã‚‰ã®ä»•æ§˜ã«ã—ã¾ã™ã‹ï¼Ÿ

â‘  æ—¥ä»˜ãŒãªã„å ´åˆã¯ä»Šæ—¥ã®æ—¥ä»˜ã§ä»£æ›¿

â‘¡ æ—¥ä»˜ãŒãªã„è¨˜äº‹ã¯RSSã‹ã‚‰é™¤å¤–

ã‚ãªãŸ:
ä»Šæ—¥ã®æ—¥ä»˜ã§ã¯ãªãã¦ãŸã linkã ã‘ã«ã—ã¦permalinkã‚‚trueã«ã—ã¦
ChatGPT:
ãªã‚‹ã»ã©ã€
pub_date ãŒ None ã®å ´åˆã¯ GUIDã‚’ãƒªãƒ³ã‚¯ãã®ã‚‚ã®ã«ã—ã¦ permalink=Trueã€
pubDate ã¯ä»˜ã‘ãšã«RSSã«å«ã‚ã‚‹ã‚ˆã†ã«ã™ã‚Œã°ã„ã„ã§ã™ã­ã€‚

ä¿®æ­£ç‰ˆã¯ã“ã†ãªã‚Šã¾ã™ã€‚

def generate_rss(items, output_path, base_url, gakkai_name):
    fg = FeedGenerator()
    fg.title(f"{gakkai_name}ãƒˆãƒ”ãƒƒã‚¯ã‚¹")
    fg.link(href=base_url)
    fg.description(f"{gakkai_name}ã®æœ€æ–°ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±")
    fg.language("ja")
    fg.generator("python-feedgen")
    fg.docs("http://www.rssboard.org/rss-specification")
    fg.lastBuildDate(datetime.now(timezone.utc))

    for item in items:
        entry = fg.add_entry()
        entry.title(item['title'])
        entry.link(href=item['link'])
        entry.description(item['description'])

        if item['pub_date'] is not None:
            guid_value = f"{item['link']}#{item['pub_date'].strftime('%Y%m%d')}"
            entry.guid(guid_value, permalink=False)
            entry.pubDate(item['pub_date'])
        else:
            # æ—¥ä»˜ãŒãªã„å ´åˆã¯ãƒªãƒ³ã‚¯ãã®ã‚‚ã®ã‚’GUIDã«ã—ã¦permalink=True
            entry.guid(item['link'], permalink=True)
            # pubDateã¯è¨­å®šã—ãªã„

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fg.rss_file(output_path)
    print(f"\nâœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼ğŸ“„ ä¿å­˜å…ˆ: {output_path}")
ã“ã‚Œãªã‚‰ã€æ—¥ä»˜ãŒãªã„è¨˜äº‹ã§ã‚‚å®‰å…¨ã«RSSç”Ÿæˆã§ãã¾ã™ã—ã€
GUIDãŒãƒªãƒ³ã‚¯ã®ã¿ï¼ˆpermalink=Trueï¼‰ã«ãªã‚‹ã®ã§RSSãƒªãƒ¼ãƒ€ãƒ¼å´ã§ã‚‚ä¸€æ„ã«åˆ¤åˆ¥ã§ãã¾ã™ã€‚

ã“ã®ä»•æ§˜ã ã¨ã€extract_items() ã¯ãã®ã¾ã¾ pub_date=None ã‚’è¿”ã—ã¦OKã§ã™ã€‚
å¸Œæœ›ãŒã‚ã‚Œã°ã€extract_items() å´ã¨åˆã‚ã›ãŸå®Œæˆç‰ˆã®ã‚³ãƒ¼ãƒ‰ã«ã¾ã¨ã‚ã¾ã™ãŒãã†ã—ã¾ã™ã‹ï¼Ÿ

ã‚ãªãŸ:
s selector "". Did you mean to CSS.escape it?
Call log:
  - waiting for  >> nth=9


âš  æ—¥ä»˜ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ
None
Traceback (most recent call last):
  File "C:\Users\sato.sota\Desktop\Gakkai_2\RSS18.py", line 79, in <module>
    generate_rss(items, rss_path, BASE_URL, GAKKAI)
  File "C:\Users\SATO~1.SOT\AppData\Local\Temp\shared-python-env\rss_utils.py", line 2, in generate_rss
    fg = FeedGenerator()
         ^^^^^^^^^^^^^
NameError: name 'FeedGenerator' is not defined
ChatGPT:

æ€è€ƒæ™‚é–“: 25s
ã‚¨ãƒ©ãƒ¼ã¯2ã¤ã‚ã‚Šã¾ã™ã€‚é †ç•ªã«ç›´ã—ã¾ã—ã‚‡ã€‚

1) NameError: name 'FeedGenerator' is not defined
generate_rss ã‚’å¤–éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆrss_utils.pyï¼‰ã«å‡ºã—ãŸã®ã«ã€ãã“ã§ FeedGenerator ã‚’ import ã—ã¦ã„ãªã„ã®ãŒåŸå› ã§ã™ã€‚rss_utils.py ã®å…ˆé ­ã«ã“ã‚Œã‚’è¶³ã—ã¦ãã ã•ã„ã€‚

# rss_utils.py
from feedgen.feed import FeedGenerator
from datetime import datetime, timezone
import os

def generate_rss(items, output_path, base_url, gakkai_name):
    fg = FeedGenerator()
    ...
ã¤ã„ã§ã«ã€pip install feedgen æ¸ˆã¿ã‹ã‚‚ç¢ºèªã‚’ã€‚

2) Playwright ã®ã€Œselector "" Did you mean to CSS.escape it?ã€
ã“ã‚Œã¯ ç©ºæ–‡å­—ã®ã‚»ãƒ¬ã‚¯ã‚¿ ã‚’æ¸¡ã—ã¦ã—ã¾ã£ãŸã¨ãã«å‡ºã‚„ã™ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã™ã€‚
ä»Šå›ã®ã‚³ãƒ¼ãƒ‰ã ã¨ã€æœ€ä¸Šéƒ¨ã®

page.wait_for_selector(SELECTOR_TITLE, timeout=10000)
blocks1 = page.locator(SELECTOR_TITLE)
ã§ SELECTOR_TITLE ãŒ ""ï¼ˆç©ºï¼‰ã ã¨å³ã‚¢ã‚¦ãƒˆã§ã™ã€‚
ã¾ãŸã€blocks2 = page.locator(SELECTOR_DATE) ã‚‚åŒæ§˜ã€‚

ã•ã‚‰ã«ã€waiting for >> nth=9 ã¯ ã‚¿ã‚¤ãƒˆãƒ«ã¨æ—¥ä»˜ã®ä»¶æ•°ãŒã‚ºãƒ¬ã¦ã„ã‚‹ ã¨ãã«ã€blocks2.nth(i) ãŒå­˜åœ¨ã—ãªã„è¦ç´ ã‚’å¾…ã¡ç¶šã‘ã‚‹æ™‚ã«ã‚‚å‡ºãŒã¡ã§ã™ã€‚

å¯¾ç­–ï¼ˆå …ç‰¢åŒ–ãƒ‘ãƒƒãƒï¼‰
ã‚»ãƒ¬ã‚¯ã‚¿å¼•æ•°ãŒ None ã‚„ç©ºæ–‡å­—ãªã‚‰ä¾‹å¤–ã«ã™ã‚‹ï¼ˆæœ€ä½é™ TITLE ã¯å¿…é ˆï¼‰ã€‚

ã‚¿ã‚¤ãƒˆãƒ«ã®ä»¶æ•° ã¨ æ—¥ä»˜ã®ä»¶æ•° ã®æœ€å°å€¤ã§ãƒ«ãƒ¼ãƒ—ã™ã‚‹ã€‚

date_selector ãŒç©ºã®ã¨ãã¯ block2.inner_text() ã‚’ä½¿ã†ï¼ˆæ—¢ã«ãã†ã—ã¦ã„ã‚‹ãŒã€ä»¶æ•°ã‚ºãƒ¬å¯¾ç­–ãŒå¿…è¦ï¼‰ã€‚

å·®åˆ†ã‚¤ãƒ¡ãƒ¼ã‚¸ã¯ã“ã‚“ãªæ„Ÿã˜ã§ã™ã€‚

def extract_items(page, SELECTOR_DATE, SELECTOR_TITLE, 
                  title_selector, title_index, 
                  href_selector, href_index, 
                  base_url, date_selector, date_index, 
                  date_format, date_regex, max_items=10):

    # 0) ã‚»ãƒ¬ã‚¯ã‚¿ã®äº‹å‰ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if not SELECTOR_TITLE or not str(SELECTOR_TITLE).strip():
        raise ValueError("SELECTOR_TITLE ãŒç©ºã§ã™ã€‚å¿…é ˆã‚»ãƒ¬ã‚¯ã‚¿ãªã®ã§æ­£ã—ã„CSS/XPathã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    if SELECTOR_DATE is None:
        raise ValueError("SELECTOR_DATE ãŒ None ã§ã™ã€‚æ—¥ä»˜ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã¯ç©ºæ–‡å­—ã§ãªãã€æ­£ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ã‚’æ¸¡ã—ã¦ãã ã•ã„ã€‚")

    page.wait_for_selector(SELECTOR_TITLE, timeout=10000)

    blocks1 = page.locator(SELECTOR_TITLE)
    blocks2 = page.locator(SELECTOR_DATE)

    count1 = blocks1.count()
    count2 = blocks2.count()
    count = min(count1, count2, max_items)

    print(f"ğŸ“¦ ç™ºè¦‹ã—ãŸè¨˜äº‹æ•°(ã‚¿ã‚¤ãƒˆãƒ«): {count1}")
    print(f"ğŸ“… ç™ºè¦‹ã—ãŸæ—¥ä»˜ãƒ–ãƒ­ãƒƒã‚¯æ•°: {count2}")
    print(f"ğŸ” è§£æå¯¾è±¡ä»¶æ•°: {count}")

    items = []

    for i in range(count):
        try:
            block1 = blocks1.nth(i)
            block2 = blocks2.nth(i)

            # ã‚¿ã‚¤ãƒˆãƒ«
            if title_selector:
                title_elem = block1.locator(title_selector).nth(title_index)
                title = title_elem.inner_text().strip()
            else:
                title = block1.inner_text().strip()
            print(title)

            # URL
            if title_selector:
                try:
                    href = block1.locator(href_selector).nth(href_index).get_attribute("href")
                except:
                    href = block1.get_attribute("href")  # å¿µã®ãŸã‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            else:
                href = block1.get_attribute("href")

            full_link = urljoin(base_url, href) if href else base_url
            print(full_link)

            # æ—¥ä»˜
            if date_selector:
                try:
                    date_text = block2.locator(date_selector).nth(date_index).inner_text().strip()
                except Exception as e:
                    print(f"âš  æ—¥ä»˜ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã«ã‚ˆã‚‹å–å¾—ã«å¤±æ•—: {e}")
                    date_text = ""
            else:
                try:
                    date_text = block2.inner_text().strip()
                except Exception as e:
                    print(f"âš  ç›´æ¥æ—¥ä»˜å–å¾—ã«å¤±æ•—: {e}")
                    date_text = ""
            print(date_text)

            match = re.search(date_regex, date_text)
            if match:
                year_str, month_str, day_str = match.groups()
                year = int(year_str)
                if year < 100:
                    year += 2000
                pub_date = datetime(year, int(month_str), int(day_str), tzinfo=timezone.utc)
            else:
                print("âš  æ—¥ä»˜ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                pub_date = None

            print(pub_date)

            if not title or not href:
                print(f"âš  å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæ¬ è½ã—ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ{i+1}è¡Œç›®ï¼‰: title='{title}', href='{href}'")
                continue

            items.append({
                "title": title,
                "link": full_link,
                "description": title,
                "pub_date": pub_date
            })

        except Exception as e:
            print(f"âš  è¡Œ{i+1}ã®è§£æã«å¤±æ•—: {e}")
            continue

    return items
3) pub_date=None ã®RSSå¯¾å¿œï¼ˆã”æŒ‡å®šã®ä»•æ§˜ï¼‰
å…ˆã»ã©ã®åˆæ„ã©ãŠã‚Šã€æ—¥ä»˜ãªã—ã¯ GUID=linkï¼ˆpermalink=Trueï¼‰ã€pubDateã¯ä»˜ã‘ãªã„ ã«ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚rss_utils.py ã® generate_rss ã¯ã“ã®å½¢ã§ã€‚

from feedgen.feed import FeedGenerator
from datetime import datetime, timezone
import os

def generate_rss(items, output_path, base_url, gakkai_name):
    fg = FeedGenerator()
    fg.title(f"{gakkai_name}ãƒˆãƒ”ãƒƒã‚¯ã‚¹")
    fg.link(href=base_url)
    fg.description(f"{gakkai_name}ã®æœ€æ–°ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±")
    fg.language("ja")
    fg.generator("python-feedgen")
    fg.docs("http://www.rssboard.org/rss-specification")
    fg.lastBuildDate(datetime.now(timezone.utc))

    for item in items:
        entry = fg.add_entry()
        entry.title(item['title'])
        entry.link(href=item['link'])
        entry.description(item['description'])

        if item['pub_date'] is not None:
            guid_value = f"{item['link']}#{item['pub_date'].strftime('%Y%m%d')}"
            entry.guid(guid_value, permalink=False)
            entry.pubDate(item['pub_date'])
        else:
            entry.guid(item['link'], permalink=True)
            # pubDateã¯è¨­å®šã—ãªã„

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fg.rss_file(output_path)
    print(f"\nâœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼ğŸ“„ ä¿å­˜å…ˆ: {output_path}")
ã“ã‚Œã§

FeedGenerator ã® NameError ã¯è§£æ¶ˆ

ç©ºã‚»ãƒ¬ã‚¯ã‚¿/ä»¶æ•°ã‚ºãƒ¬ã«ã‚ˆã‚‹ Playwright ã‚¨ãƒ©ãƒ¼ã‚’å›é¿

pub_date=None ã§ã‚‚RSSç”ŸæˆOK

ã®3ç‚¹ãŒã¾ã¨ã¾ã£ã¦ç›´ã›ã¾ã™ã€‚å¿…è¦ãªã‚‰ã€å®Ÿéš›ã®ã‚»ãƒ¬ã‚¯ã‚¿å€¤ï¼ˆSELECTOR_TITLE / SELECTOR_DATE / title_selector / date_selectorï¼‰ã‚’æ•™ãˆã¦ãã‚Œã‚Œã°ã€ã‚‚ã†ä¸€æ­©çªã£è¾¼ã‚“ã ã‚»ãƒ¬ã‚¯ã‚¿èª¿æ•´ã‚‚ã‚„ã‚Šã¾ã™ã€‚

ã‚ãªãŸ:
<a href="topic/20250807.pdf" target="_blank">äº¬éƒ½å¤§å­¦ã€€æ•™å“¡å…¬å‹Ÿã®ãŠçŸ¥ã‚‰ã›ï¼ˆ2025/8/7ï¼‰</a>

SELECTOR_DATE = "div#main_left li"
date_selector = "a"
date_index = 0
year_unit = "/"
month_unit = "/"
day_unit = ""
date_format = f"%Y{year_unit}%m{month_unit}%d{day_unit}"
date_regex = rf"(\d{{2,4}}){year_unit}(\d{{1,2}}){month_unit}(\d{{1,2}}){day_unit}"

ã“ã‚Œã§æ—¥ä»˜éƒ¨åˆ†æ­£è¦è¡¨ç¾ã§å–ã‚Šå‡ºã›ã‚‹ã¯ãšã ã‚ˆã­ï¼Ÿ
ChatGPT:
ã¯ã„ã€ãã®HTMLã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã‚‰ã€ãã® date_regex ã§æ—¥ä»˜éƒ¨åˆ†ã¯ã¡ã‚ƒã‚“ã¨å–ã‚Œã‚‹ã¯ãšã§ã™ã€‚

ç†ç”±
ã‚ãªãŸã®HTML:

<a href="topic/20250807.pdf" target="_blank">
  äº¬éƒ½å¤§å­¦ã€€æ•™å“¡å…¬å‹Ÿã®ãŠçŸ¥ã‚‰ã›ï¼ˆ2025/8/7ï¼‰
</a>
ã“ã®ä¸­ã®æ—¥ä»˜éƒ¨åˆ†ã¯ 2025/8/7 ã§ã™ã€‚

ã‚ãªãŸã® date_regex:

rf"(\d{{2,4}}){year_unit}(\d{{1,2}}){month_unit}(\d{{1,2}}){day_unit}"
year_unit = "/"
month_unit = "/"
day_unit = ""

ãªã®ã§å±•é–‹å¾Œã¯:

r"(\d{2,4})/(\d{1,2})/(\d{1,2})"
ç¢ºèª
import re
html_text = 'äº¬éƒ½å¤§å­¦ã€€æ•™å“¡å…¬å‹Ÿã®ãŠçŸ¥ã‚‰ã›ï¼ˆ2025/8/7ï¼‰'
date_regex = r"(\d{2,4})/(\d{1,2})/(\d{1,2})"

match = re.search(date_regex, html_text)
if match:
    print(match.groups())
å‡ºåŠ›:

('2025', '8', '7')
è£œè¶³
ã‚‚ã— re.search ã§ãƒãƒƒãƒã—ãªã„å ´åˆã¯ã€
HTMLå†…ã®å…¨è§’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚„å…¨è§’æ•°å­—ã€ã‚ã‚‹ã„ã¯å…¨è§’æ‹¬å¼§ãŒæ··ã–ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
ç‰¹ã«å…¨è§’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ "ï¼" ã ã¨ "/" ã®æ­£è¦è¡¨ç¾ã«ã¯ä¸€è‡´ã—ã¾ã›ã‚“ã€‚

ãã®å ´åˆã¯ã€äº‹å‰ã«å…¨è§’â†’åŠè§’å¤‰æ›ã™ã‚‹ã¨å®‰å®šã—ã¾ã™ã€‚

import unicodedata
text = unicodedata.normalize("NFKC", html_text)
å¿…è¦ãªã‚‰ã€ç§ã®ã»ã†ã§ã“ã®æ­£è¦è¡¨ç¾ã‚’å…¨è§’æ•°å­—ã‚„å…¨è§’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã«ã‚‚å¯¾å¿œã™ã‚‹å½¢ã«ç›´ã™ã“ã¨ã‚‚ã§ãã¾ã™ã€‚
ãã†ã—ã¾ã™ã‹ï¼Ÿ

ã‚ãªãŸ:
import re
from datetime import datetime, timezone
from urllib.parse import urljoin

def extract_items(page, SELECTOR_DATE, SELECTOR_TITLE, title_selector, title_index, href_selector, href_index, base_url, date_selector, date_index, date_format, date_regex, max_items=10):
    
    page.wait_for_selector(SELECTOR_TITLE, timeout=10000)
    
    blocks1 = page.locator(SELECTOR_TITLE)
    count = blocks1.count()
    
    print(f"ğŸ“¦ ç™ºè¦‹ã—ãŸè¨˜äº‹æ•°: {count}")
    items = []

    blocks2 = page.locator(SELECTOR_DATE)
    
    for i in range(min(count, max_items)):
        try:
            block1 = blocks1.nth(i)
            block2 = blocks2.nth(i)

            # ã‚¿ã‚¤ãƒˆãƒ«
            if title_selector:
                title_elem = block1.locator(title_selector).nth(title_index)
                title = title_elem.inner_text().strip()
            else:
                title = block1.inner_text().strip()
            print(title)


            
            # URL
            if title_selector:
                try:
                    href = block1.locator(href_selector).nth(href_index).get_attribute("href")
                    full_link = urljoin(base_url, href)
                except:
                    href = ""
                    full_link = base_url
            else:
                try:
                    href = block1.get_attribute("href")
                    full_link = urljoin(base_url, href)
                except:
                    href = ""
                    full_link = base_url
            print(full_link)
            
            # æ—¥ä»˜
            # date_selector ãŒç©ºæ–‡å­—ã‚„ None ã§ãªã„å ´åˆ â†’ å­è¦ç´ æ¢ç´¢ã€ãã‚Œä»¥å¤–ã¯ãã®ã¾ã¾
            if date_selector:
                try:
                    date_text = block2.locator(date_selector).nth(date_index).inner_text().strip()
                except Exception as e:
                    print(f"âš  æ—¥ä»˜ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã«ã‚ˆã‚‹å–å¾—ã«å¤±æ•—: {e}")
                    date_text = ""
            else:
                try:
                    date_text = block2.inner_text().strip()
                except Exception as e:
                    print(f"âš  ç›´æ¥æ—¥ä»˜å–å¾—ã«å¤±æ•—: {e}")
                    date_text = ""
            print(date_text)
            match = re.search(date_regex, date_text)

            if match:
                year_str, month_str, day_str = match.groups()
                year = int(year_str)
                if year < 100:
                    year += 2000  # 2æ¡è¥¿æš¦ â†’ 2000å¹´ä»¥é™ã¨ä»®å®š
                pub_date = datetime(year, int(month_str), int(day_str), tzinfo=timezone.utc)
            else:
                print("âš  æ—¥ä»˜ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                pub_date = None  # or continue
            
            # match = re.search(date_regex,date_text)
            # if match:
            #     date_str = match.group()
            #     pub_date = datetime.strptime(date_str, date_format).replace(tzinfo=timezone.utc)
            # else:
            #     print("âš  æ—¥ä»˜ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")

            print(pub_date)
            
            if not title or not href:
                print(f"âš  å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæ¬ è½ã—ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ{i+1}è¡Œç›®ï¼‰: title='{title}', href='{href}'")
                continue

            items.append({
                "title": title,
                "link": full_link,
                "description": title,
                "pub_date": pub_date
            })

        except Exception as e:
            print(f"âš  è¡Œ{i+1}ã®è§£æã«å¤±æ•—: {e}")
            continue

    return items

ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’åŸºè»¸ã¨ã—ã¦


from feedgen.feed import FeedGenerator
from datetime import datetime, timezone
from urllib.parse import urljoin
import os
import re
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

BASE_URL = "https://jasweb.or.jp/"
GAKKAI = "æ—¥æœ¬å–˜æ¯å­¦ä¼š"

selector = "dd"
TITLE_SELECTOR = "a"
TITLE_index= 0
href_SELECTOR = "a"
href_index = 0

def generate_rss(items, output_path):
    fg = FeedGenerator()
    fg.title(f"{GAKKAI}ãƒˆãƒ”ãƒƒã‚¯ã‚¹")
    fg.link(href=BASE_URL)
    fg.description(f"{GAKKAI}ã®æœ€æ–°ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±")
    fg.language("ja")
    fg.generator("python-feedgen")
    fg.docs("http://www.rssboard.org/rss-specification")
    fg.lastBuildDate(datetime.now(timezone.utc))

    for item in items:
        entry = fg.add_entry()
        entry.title(item['title'])
        entry.link(href=item['link'])
        entry.description(item['description'])
        entry.guid(item['link'], permalink=True)
        entry.pubDate(item['pub_date'])

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fg.rss_file(output_path)
    print(f"\nâœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼ğŸ“„ ä¿å­˜å…ˆ: {output_path}")

def extract_items(page):
    # iframeã‚’å¾…æ©Ÿã—ã¦å–å¾—
    page.wait_for_selector("iframe", timeout=10000)
    iframe_element = page.locator("iframe").first.element_handle()

    if iframe_element is None:
        print("âš  iframeãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return []

    frame = iframe_element.content_frame()
    if frame is None:
        print("âš  iframeã®ä¸­èº«ï¼ˆframeï¼‰ãŒã¾ã èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        return []

    frame.wait_for_selector(selector, timeout=10000)

    blocks = frame.locator(selector)
    count = blocks.count()
    print(f"ğŸ“¦ ç™ºè¦‹ã—ãŸè¨˜äº‹æ•°: {count}")
    items = []

    max_items = 10
    for i in range(min(count, max_items)):
        try:
            block = blocks.nth(i)
            pub_date = datetime.now(timezone.utc)
            title = block.locator(TITLE_SELECTOR).nth(TITLE_index).inner_text().strip()

            try:
                href = block.locator(href_SELECTOR).nth(href_index).get_attribute("href")
                full_link = urljoin(BASE_URL, href)
            except:
                href = ""
                full_link = BASE_URL

            if not title or not href:
                print(f"âš  å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæ¬ è½ã—ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ{i+1}è¡Œç›®ï¼‰: title='{title}', href='{href}'")
                continue

            items.append({
                "title": title,
                "link": full_link,
                "description": title,
                "pub_date": pub_date
            })

        except Exception as e:
            print(f"âš  è¡Œ{i+1}ã®è§£æã«å¤±æ•—: {e}")
            continue

    return items

with sync_playwright() as p:
    print("â–¶ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ä¸­...")
    browser = p.chromium.launch(headless=True)
    context = browser.new_context()
    page = context.new_page()

    try:
        print("â–¶ ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
        page.goto(BASE_URL, timeout=30000)
        page.wait_for_load_state("load", timeout=30000)
    except PlaywrightTimeoutError:
        print("âš  ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        browser.close()
        exit()

    print("â–¶ è¨˜äº‹ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...")
    items = extract_items(page)

    if not items:
        print("âš  æŠ½å‡ºã§ããŸè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚HTMLæ§‹é€ ãŒå¤‰ã‚ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    rss_path = "rss_output/Feed3.xml"
    generate_rss(items, rss_path)
    browser.close()

ã“ã®ã‚³ãƒ¼ãƒ‰ã®è©²å½“éƒ¨åˆ†ã®Iframeã«é–¢ã—ã¦å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«å¤‰æ›´ã—ãŸã„


ChatGPT:

æ€è€ƒæ™‚é–“: 21s
ã„ã„ã­ã€iframeå†…ã§ã‚‚å‹•ãæ±ç”¨ç‰ˆã«ã—ã¦ãŠãã¾ã™ã€‚
extract_items ã« iframe_selectorï¼ˆçœç•¥å¯ï¼‰ã‚’è¿½åŠ ã—ã¦ã€æŒ‡å®šãŒã‚ã‚Œã°ãã® iframe ã® Frame ä¸Šã§ã‚»ãƒ¬ã‚¯ã‚¿æ¢ç´¢ã‚’è¡Œã†ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚æœªæŒ‡å®šãªã‚‰å¾“æ¥ã©ãŠã‚Šãƒšãƒ¼ã‚¸ç›´ä¸‹ã‚’æ¢ç´¢ã—ã¾ã™ã€‚ã¤ã„ã§ã« pub_date=None ã®ã¨ãã¯ guid=link(permalink=True) ã«ã—ã¦ pubDate ã¯ä»˜ã‘ãªã„ã‚ˆã†ã« generate_rss ã‚‚èª¿æ•´ã—ã¦ã„ã¾ã™ã€‚

import re
from datetime import datetime, timezone
from urllib.parse import urljoin
from playwright.sync_api import TimeoutError as PlaywrightTimeoutError

def _get_root_context(page, iframe_selector=None, iframe_index=0, timeout=10000):
    """
    iframe_selector ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ã€ãã® iframe ã® content_frame ã‚’è¿”ã™ã€‚
    æŒ‡å®šãŒç„¡ã‘ã‚Œã° page ã‚’ãã®ã¾ã¾è¿”ã™ã€‚
    """
    if not iframe_selector:
        return page  # ãƒšãƒ¼ã‚¸ç›´ä¸‹ã§æ¢ç´¢

    # iframe ã‚’å¾…ã£ã¦å–å¾—
    page.wait_for_selector(iframe_selector, timeout=timeout)
    iframe_loc = page.locator(iframe_selector).nth(iframe_index)
    handle = iframe_loc.element_handle()
    if handle is None:
        raise RuntimeError("iframeè¦ç´ ã®ãƒãƒ³ãƒ‰ãƒ«å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚selector / index ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    frame = handle.content_frame()
    if frame is None:
        raise RuntimeError("iframeã®content_frameãŒã¾ã èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å°‘ã—å¾…ã¤ã‹selectorã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    return frame

def extract_items(
    page,
    SELECTOR_DATE,
    SELECTOR_TITLE,
    title_selector,
    title_index,
    href_selector,
    href_index,
    base_url,
    date_selector,
    date_index,
    date_format,
    date_regex,
    max_items=10,
    *,
    iframe_selector=None,
    iframe_index=0,
    iframe_timeout=10000
):
    """
    æ±ç”¨æŠ½å‡ºã€‚iframe_selector ã‚’æ¸¡ã™ã¨ã€ãã® iframe å†…ã§ SELECTOR_* ã‚’æ¢ç´¢ã™ã‚‹ã€‚
    """
    root = _get_root_context(page, iframe_selector=iframe_selector, iframe_index=iframe_index, timeout=iframe_timeout)

    # ã¾ãšã‚¿ã‚¤ãƒˆãƒ«å´ã®å­˜åœ¨ã‚’å¾…æ©Ÿ
    root.wait_for_selector(SELECTOR_TITLE, timeout=10000)

    blocks1 = root.locator(SELECTOR_TITLE)
    blocks2 = root.locator(SELECTOR_DATE)

    count1 = blocks1.count()
    count2 = blocks2.count()
    total = min(count1, count2, max_items)

    print(f"ğŸ“¦ ç™ºè¦‹ã—ãŸè¨˜äº‹æ•°(ã‚¿ã‚¤ãƒˆãƒ«): {count1}")
    print(f"ğŸ“… ç™ºè¦‹ã—ãŸæ—¥ä»˜ãƒ–ãƒ­ãƒƒã‚¯æ•°: {count2}")
    print(f"ğŸ” è§£æå¯¾è±¡ä»¶æ•°: {total}")

    items = []

    for i in range(total):
        try:
            block1 = blocks1.nth(i)
            block2 = blocks2.nth(i)

            # ã‚¿ã‚¤ãƒˆãƒ«
            if title_selector:
                title_elem = block1.locator(title_selector).nth(title_index)
                title = title_elem.inner_text().strip()
            else:
                title = block1.inner_text().strip()
            print(title)

            # URL
            if title_selector:
                try:
                    href = block1.locator(href_selector).nth(href_index).get_attribute("href")
                except:
                    href = block1.get_attribute("href")
            else:
                href = block1.get_attribute("href")

            full_link = urljoin(base_url, href) if href else base_url
            print(full_link)

            # æ—¥ä»˜ãƒ†ã‚­ã‚¹ãƒˆ
            if date_selector:
                try:
                    date_text = block2.locator(date_selector).nth(date_index).inner_text().strip()
                except Exception as e:
                    print(f"âš  æ—¥ä»˜ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã«ã‚ˆã‚‹å–å¾—ã«å¤±æ•—: {e}")
                    date_text = ""
            else:
                try:
                    date_text = block2.inner_text().strip()
                except Exception as e:
                    print(f"âš  ç›´æ¥æ—¥ä»˜å–å¾—ã«å¤±æ•—: {e}")
                    date_text = ""
            print(date_text)

            # æ­£è¦è¡¨ç¾ã§æŠ½å‡º
            match = re.search(date_regex, date_text)
            if match:
                year_str, month_str, day_str = match.groups()
                year = int(year_str)
                if year < 100:
                    year += 2000
                pub_date = datetime(year, int(month_str), int(day_str), tzinfo=timezone.utc)
            else:
                print("âš  æ—¥ä»˜ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                pub_date = None

            print(pub_date)

            if not title or not href:
                print(f"âš  å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæ¬ è½ã—ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ{i+1}è¡Œç›®ï¼‰: title='{title}', href='{href}'")
                continue

            items.append({
                "title": title,
                "link": full_link,
                "description": title,
                "pub_date": pub_date
            })

        except Exception as e:
            print(f"âš  è¡Œ{i+1}ã®è§£æã«å¤±æ•—: {e}")
            continue

    return items
# â”€â”€ ã“ã¡ã‚‰ã¯ RSS ç”Ÿæˆå´ï¼ˆpub_date=None ã®æ™‚ã¯ guid=link, permalink=True / pubDateãªã—ï¼‰
from feedgen.feed import FeedGenerator
from datetime import datetime, timezone
import os

BASE_URL = "https://jasweb.or.jp/"
GAKKAI = "æ—¥æœ¬å–˜æ¯å­¦ä¼š"

def generate_rss(items, output_path):
    fg = FeedGenerator()
    fg.title(f"{GAKKAI}ãƒˆãƒ”ãƒƒã‚¯ã‚¹")
    fg.link(href=BASE_URL)
    fg.description(f"{GAKKAI}ã®æœ€æ–°ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±")
    fg.language("ja")
    fg.generator("python-feedgen")
    fg.docs("http://www.rssboard.org/rss-specification")
    fg.lastBuildDate(datetime.now(timezone.utc))

    for item in items:
        entry = fg.add_entry()
        entry.title(item['title'])
        entry.link(href=item['link'])
        entry.description(item['description'])

        if item['pub_date'] is not None:
            guid_value = f"{item['link']}#{item['pub_date'].strftime('%Y%m%d')}"
            entry.guid(guid_value, permalink=False)
            entry.pubDate(item['pub_date'])
        else:
            entry.guid(item['link'], permalink=True)
            # pubDate ã¯ä»˜ã‘ãªã„

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fg.rss_file(output_path)
    print(f"\nâœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼ğŸ“„ ä¿å­˜å…ˆ: {output_path}")
ä½¿ã„æ–¹ï¼ˆä»Šå›ã®ã‚µã‚¤ãƒˆä¾‹ï¼šiframe å†…ã® dd â†’ aï¼‰
# ä¾‹ï¼šäº¬éƒ½å¤§å­¦ã®ã‚ˆã†ã« iframe å†…ã® <dd><a>â€¦</a> ã‚’èª­ã‚€ã‚±ãƒ¼ã‚¹
items = extract_items(
    page,
    SELECTOR_DATE="dd",
    SELECTOR_TITLE="dd",
    title_selector="a",
    title_index=0,
    href_selector="a",
    href_index=0,
    base_url=BASE_URL,
    date_selector="a",
    date_index=0,
    date_format="%Y/%m/%d",
    date_regex=r"(\d{2,4})/(\d{1,2})/(\d{1,2})",
    max_items=10,
    iframe_selector="iframe",   # â† ã“ã“ã‚’æŒ‡å®šã™ã‚‹ã¨ iframe å†…ã§æ¢ç´¢
    iframe_index=0,
    iframe_timeout=10000
)
ã“ã‚Œã§ã€åŒã˜é–¢æ•°ã®ã¾ã¾ ç›´ä¸‹ï¼iframe ã®ã©ã¡ã‚‰ã«ã‚‚å¯¾å¿œã§ãã¾ã™ã€‚å¿…è¦ãªã‚‰ã€å…¨è§’æ··ã˜ã‚Šã®æ—¥ä»˜ï¼ˆå…¨è§’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚„å…¨è§’æ•°å­—ï¼‰ã«ã‚‚è€ãˆã‚‹å‰å‡¦ç†ï¼ˆNFKCæ­£è¦åŒ–ï¼‰ã‚’è¶³ã™ãƒ‘ãƒƒãƒã‚‚å‡ºã—ã¾ã™ã‚ˆã€‚




é¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“é¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“
ChatGPT ã®å›ç­”ã¯å¿…ãšã—ã‚‚æ­£ã—ã„ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚OpenAI ã¯ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã« CSD ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚
